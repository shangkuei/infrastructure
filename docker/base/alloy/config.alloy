// ============================================================
// Grafana Alloy Configuration - Unified Telemetry for Unraid
// ============================================================
//
// This configuration handles:
// 1. Log collection - Docker containers, push to Loki
// 2. Metrics collection - Node/container metrics, push to Prometheus
//
// Environment variables (from docker-compose):
// - CLUSTER_NAME: Cluster identifier (e.g., shangkuei-unraid)
// - LOKI_URL: Loki push endpoint (e.g., http://loki/loki/api/v1/push)
// - LOKI_TENANT_ID: Loki tenant ID (e.g., shangkuei-lab)
// - PROMETHEUS_URL: Prometheus remote_write endpoint
// - HOSTNAME: Host identifier
// - NODE_EXPORTER_URL: Node exporter metrics endpoint
// - CADVISOR_URL: cAdvisor metrics endpoint
// - IMMICH_URL: Immich metrics endpoint (optional)
// - GITEA_URL: Gitea metrics endpoint (optional)
// - GITEA_METRICS_TOKEN: Gitea metrics authentication token (optional)
//
// ============================================================

// ============================================================
// LOG COLLECTION - Docker Containers
// ============================================================

// Discover Docker containers
discovery.docker "containers" {
  host = "unix:///var/run/docker.sock"
}

// Relabel discovered containers - add labels for log queries
discovery.relabel "docker_logs" {
  targets = discovery.docker.containers.targets

  // Set container name
  rule {
    source_labels = ["__meta_docker_container_name"]
    regex         = "/(.*)"
    target_label  = "container"
  }

  // Set compose project (if using docker-compose)
  rule {
    source_labels = ["__meta_docker_container_label_com_docker_compose_project"]
    target_label  = "compose_project"
  }

  // Set compose service (if using docker-compose)
  rule {
    source_labels = ["__meta_docker_container_label_com_docker_compose_service"]
    target_label  = "compose_service"
  }

  // Set job label as compose_project/compose_service or just container name
  rule {
    source_labels = ["__meta_docker_container_label_com_docker_compose_project", "__meta_docker_container_label_com_docker_compose_service"]
    separator     = "/"
    regex         = "(.+/.+)"
    target_label  = "job"
  }

  // Fallback job label for non-compose containers
  rule {
    source_labels = ["__meta_docker_container_name", "job"]
    regex         = "/(.+);"
    target_label  = "job"
  }

  // Set image name
  rule {
    source_labels = ["__meta_docker_container_image"]
    target_label  = "image"
  }

  // Add cluster label for multi-cluster support
  rule {
    target_label = "cluster"
    replacement  = env("CLUSTER_NAME")
  }

  // Add host label
  rule {
    target_label = "host"
    replacement  = env("HOSTNAME")
  }
}

// Collect Docker container logs
loki.source.docker "containers" {
  host             = "unix:///var/run/docker.sock"
  targets          = discovery.relabel.docker_logs.output
  forward_to       = [loki.process.docker_logs.receiver]
  relabel_rules    = discovery.relabel.docker_logs.rules
  refresh_interval = "5s"
}

// Process Docker logs - parse and enrich
loki.process "docker_logs" {
  forward_to = [loki.write.loki.receiver]

  // Try to extract log level from JSON format: {"level":"info", ...}
  stage.json {
    expressions = {
      level = "level",
    }
  }

  // Fallback: extract level from logfmt style (level=info, level=INFO)
  stage.regex {
    expression = "(?i)level=(?P<level>\\w+)"
  }

  // Set level label if extracted
  stage.labels {
    values = {
      level = "",
    }
  }
}

// Push logs to Loki (multi-tenant mode)
loki.write "loki" {
  endpoint {
    url       = env("LOKI_URL")
    tenant_id = env("LOKI_TENANT_ID")
  }
}

// ============================================================
// METRICS COLLECTION - Node Exporter & cAdvisor
// ============================================================

// Scrape Node Exporter metrics (host-level metrics)
prometheus.scrape "node_exporter" {
  targets = [{
    __address__ = coalesce(env("NODE_EXPORTER_URL"), "node-exporter:9100"),
  }]
  forward_to      = [prometheus.relabel.node_exporter_filter.receiver]
  job_name        = "node-exporter"
  scrape_interval = "30s"
}

// Filter Node Exporter metrics - keep only metrics used by dashboards and alerts
// This significantly reduces cardinality by dropping unused collectors
prometheus.relabel "node_exporter_filter" {
  forward_to = [prometheus.relabel.add_labels.receiver]

  // Keep metrics used by Node Exporter dashboards, alerts, and NAS monitoring:
  // Standard metrics:
  // - node_cpu_* (CPU usage, recording rules)
  // - node_load* (load averages)
  // - node_memory_* (memory usage)
  // - node_disk_* (disk I/O)
  // - node_filesystem_* (disk space, alerts)
  // - node_network_* (network I/O, errors)
  // - node_vmstat_pgmajfault (memory saturation)
  // - node_time_seconds (cluster variable query)
  // - node_timex_* (clock skew alerts)
  // - node_uname_info (instance variable query)
  // - node_exporter_build_info (metadata)
  // - node_nf_conntrack_* (conntrack alerts)
  // - node_textfile_* (custom metrics, SMART data)
  // - node_md_* (RAID alerts - important for Unraid)
  // - node_filefd_* (file descriptor alerts)
  // - node_systemd_* (systemd service alerts)
  // - node_bonding_* (network bonding alerts)
  // NAS-specific metrics:
  // - node_hwmon_* (temperature sensors - CPU, disks)
  // - node_thermal_* (thermal zone info)
  // - node_cooling_* (cooling device state)
  // - node_power_supply_* (UPS/power info if available)
  rule {
    source_labels = ["__name__"]
    regex         = "(node_(cpu|load|memory|disk|filesystem|network|vmstat_pgmajfault|time_seconds|timex|uname_info|exporter_build_info|nf_conntrack|textfile|md|filefd|systemd|bonding|hwmon|thermal|cooling|power_supply).*|up)"
    action        = "keep"
  }
}

// Scrape cAdvisor metrics (container metrics)
prometheus.scrape "cadvisor" {
  targets = [{
    __address__ = coalesce(env("CADVISOR_URL"), "cadvisor:8080"),
  }]
  forward_to      = [prometheus.relabel.cadvisor_filter.receiver]
  job_name        = "cadvisor"
  scrape_interval = "30s"
  metrics_path    = "/metrics"
}

// Filter cAdvisor metrics - keep only metrics used by Docker containers dashboard
// This significantly reduces cardinality by dropping unused metrics
prometheus.relabel "cadvisor_filter" {
  forward_to = [prometheus.relabel.add_labels.receiver]

  // Keep only the container metrics used by the Docker containers dashboard:
  // - container_cpu_usage_seconds_total (CPU usage)
  // - container_memory_usage_bytes (memory usage)
  // - container_network_receive_bytes_total (network RX)
  // - container_network_transmit_bytes_total (network TX)
  // - container_fs_reads_bytes_total (disk read I/O)
  // - container_fs_writes_bytes_total (disk write I/O)
  rule {
    source_labels = ["__name__"]
    regex         = "container_(cpu_usage_seconds_total|memory_usage_bytes|network_(receive|transmit)_bytes_total|fs_(reads|writes)_bytes_total)"
    action        = "keep"
  }
}

// ============================================================
// APPLICATION METRICS - Immich & Gitea (Optional)
// ============================================================

// Scrape Immich metrics (photo management)
// Set IMMICH_URL environment variable to enable
prometheus.scrape "immich" {
  targets = [{
    __address__ = coalesce(env("IMMICH_URL"), ""),
  }]
  forward_to      = [prometheus.relabel.add_labels.receiver]
  job_name        = "immich"
  scrape_interval = "30s"
  metrics_path    = "/metrics"
}

// Scrape Gitea metrics (git hosting)
// Set GITEA_URL and GITEA_METRICS_TOKEN environment variables to enable
prometheus.scrape "gitea" {
  targets = [{
    __address__ = coalesce(env("GITEA_URL"), ""),
  }]
  forward_to      = [prometheus.relabel.add_labels.receiver]
  job_name        = "gitea"
  scrape_interval = "30s"
  metrics_path    = "/metrics"

  // Bearer token authentication for Gitea metrics
  authorization {
    type        = "Bearer"
    credentials = coalesce(env("GITEA_METRICS_TOKEN"), "")
  }
}

// Add cluster and host labels to all metrics
prometheus.relabel "add_labels" {
  forward_to = [prometheus.remote_write.prometheus.receiver]

  rule {
    target_label = "cluster"
    replacement  = env("CLUSTER_NAME")
  }

  rule {
    target_label = "host"
    replacement  = env("HOSTNAME")
  }
}

// Push metrics to Prometheus via remote_write
prometheus.remote_write "prometheus" {
  endpoint {
    url = env("PROMETHEUS_URL")
  }
}
