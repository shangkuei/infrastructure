// ============================================================
// Grafana Alloy Configuration - Unified Telemetry for Unraid
// ============================================================
//
// This configuration handles:
// 1. Log collection - Docker containers, push to Loki
// 2. Metrics collection - Node/container metrics, push to Prometheus
//
// Environment variables (from docker-compose):
// - CLUSTER_NAME: Cluster identifier (e.g., shangkuei-unraid)
// - LOKI_URL: Loki push endpoint (e.g., http://loki/loki/api/v1/push)
// - LOKI_TENANT_ID: Loki tenant ID (e.g., shangkuei-lab)
// - PROMETHEUS_URL: Prometheus remote_write endpoint
// - HOSTNAME: Host identifier
// - NODE_EXPORTER_URL: Node exporter metrics endpoint
// - CADVISOR_URL: cAdvisor metrics endpoint
// - CLOUDFLARED_URL: Cloudflared metrics endpoint (optional)
// - VAULTWARDEN_URL: Vaultwarden metrics endpoint (optional, via vwmetrics)
// - IMMICH_URL: Immich metrics endpoint (optional)
// - GITEA_URL: Gitea metrics endpoint (optional)
// - GITEA_METRICS_TOKEN: Gitea metrics authentication token (optional)
// - PLEX_URL: Plex metrics endpoint (optional, via plex-media-server-exporter)
// - CODE_SERVER_URL: code-server metrics endpoint (optional, via process-exporter)
//
// ============================================================

// ============================================================
// LOG COLLECTION - Docker Containers
// ============================================================

// Discover Docker containers
discovery.docker "containers" {
  host = "unix:///var/run/docker.sock"
}

// Relabel discovered containers - add labels for log queries
discovery.relabel "docker_logs" {
  targets = discovery.docker.containers.targets

  // Set container name
  rule {
    source_labels = ["__meta_docker_container_name"]
    regex         = "/(.*)"
    target_label  = "container"
  }

  // Set compose project (if using docker-compose)
  rule {
    source_labels = ["__meta_docker_container_label_com_docker_compose_project"]
    target_label  = "compose_project"
  }

  // Set compose service (if using docker-compose)
  rule {
    source_labels = ["__meta_docker_container_label_com_docker_compose_service"]
    target_label  = "compose_service"
  }

  // Set job label as compose_project/compose_service or just container name
  rule {
    source_labels = ["__meta_docker_container_label_com_docker_compose_project", "__meta_docker_container_label_com_docker_compose_service"]
    separator     = "/"
    regex         = "(.+/.+)"
    target_label  = "job"
  }

  // Fallback job label for non-compose containers
  rule {
    source_labels = ["__meta_docker_container_name", "job"]
    regex         = "/(.+);"
    target_label  = "job"
  }

  // Set image name
  rule {
    source_labels = ["__meta_docker_container_image"]
    target_label  = "image"
  }

  // Add cluster label for multi-cluster support
  rule {
    target_label = "cluster"
    replacement  = env("CLUSTER_NAME")
  }

  // Add host label
  rule {
    target_label = "host"
    replacement  = env("HOSTNAME")
  }
}

// Collect Docker container logs
loki.source.docker "containers" {
  host             = "unix:///var/run/docker.sock"
  targets          = discovery.relabel.docker_logs.output
  forward_to       = [loki.process.docker_logs.receiver]
  relabel_rules    = discovery.relabel.docker_logs.rules
  refresh_interval = "5s"
}

// Process Docker logs - parse and enrich
loki.process "docker_logs" {
  forward_to = [loki.write.loki.receiver]

  // Try to extract log level from JSON format: {"level":"info", ...}
  stage.json {
    expressions = {
      level = "level",
    }
  }

  // Fallback: extract level from logfmt style (level=info, level=INFO)
  stage.regex {
    expression = "(?i)level=(?P<level>\\w+)"
  }

  // Set level label if extracted
  stage.labels {
    values = {
      level = "",
    }
  }
}

// Push logs to Loki (multi-tenant mode)
loki.write "loki" {
  endpoint {
    url       = env("LOKI_URL")
    tenant_id = env("LOKI_TENANT_ID")
  }
}

// ============================================================
// METRICS COLLECTION - Node Exporter & cAdvisor
// ============================================================

// Scrape Node Exporter metrics (host-level metrics)
prometheus.scrape "node_exporter" {
  targets = [{
    __address__ = coalesce(env("NODE_EXPORTER_URL"), "node-exporter:9100"),
  }]
  forward_to      = [prometheus.relabel.node_exporter_filter.receiver]
  job_name        = "node-exporter"
  scrape_interval = "30s"
}

// Filter Node Exporter metrics - keep only metrics used by dashboards and alerts
// This significantly reduces cardinality by dropping unused collectors
prometheus.relabel "node_exporter_filter" {
  forward_to = [prometheus.relabel.add_labels.receiver]

  // Keep metrics used by Node Exporter dashboards, alerts, and NAS monitoring:
  // Standard metrics:
  // - node_cpu_* (CPU usage, recording rules)
  // - node_load* (load averages)
  // - node_memory_* (memory usage)
  // - node_disk_* (disk I/O)
  // - node_filesystem_* (disk space, alerts)
  // - node_network_* (network I/O, errors)
  // - node_vmstat_pgmajfault (memory saturation)
  // - node_time_seconds (cluster variable query)
  // - node_timex_* (clock skew alerts)
  // - node_uname_info (instance variable query)
  // - node_exporter_build_info (metadata)
  // - node_nf_conntrack_* (conntrack alerts)
  // - node_textfile_* (custom metrics, SMART data)
  // - node_md_* (RAID alerts - important for Unraid)
  // - node_filefd_* (file descriptor alerts)
  // - node_systemd_* (systemd service alerts)
  // - node_bonding_* (network bonding alerts)
  // NAS-specific metrics:
  // - node_hwmon_* (temperature sensors - CPU, disks)
  // - node_thermal_* (thermal zone info)
  // - node_cooling_* (cooling device state)
  // - node_power_supply_* (UPS/power info if available)
  rule {
    source_labels = ["__name__"]
    regex         = "(node_(cpu|load|memory|disk|filesystem|network|vmstat_pgmajfault|time_seconds|timex|uname_info|exporter_build_info|nf_conntrack|textfile|md|filefd|systemd|bonding|hwmon|thermal|cooling|power_supply).*|up)"
    action        = "keep"
  }
}

// Scrape cAdvisor metrics (container metrics)
prometheus.scrape "cadvisor" {
  targets = [{
    __address__ = coalesce(env("CADVISOR_URL"), "cadvisor:8080"),
  }]
  forward_to      = [prometheus.relabel.cadvisor_filter.receiver]
  job_name        = "cadvisor"
  scrape_interval = "30s"
  metrics_path    = "/metrics"
}

// Filter cAdvisor metrics - keep only metrics used by Docker containers dashboard
// This significantly reduces cardinality by dropping unused metrics
prometheus.relabel "cadvisor_filter" {
  forward_to = [prometheus.relabel.add_labels.receiver]

  // Keep only the container metrics used by the Docker containers dashboard:
  // - container_cpu_usage_seconds_total (CPU usage)
  // - container_memory_usage_bytes (memory usage)
  // - container_network_receive_bytes_total (network RX)
  // - container_network_transmit_bytes_total (network TX)
  // - container_fs_reads_bytes_total (disk read I/O)
  // - container_fs_writes_bytes_total (disk write I/O)
  rule {
    source_labels = ["__name__"]
    regex         = "container_(cpu_usage_seconds_total|memory_usage_bytes|network_(receive|transmit)_bytes_total|fs_(reads|writes)_bytes_total)"
    action        = "keep"
  }
}

// ============================================================
// APPLICATION METRICS - Docker Services (Optional)
// ============================================================

// Scrape Cloudflared metrics (Cloudflare Tunnel)
// Set CLOUDFLARED_URL environment variable to enable
prometheus.scrape "cloudflared" {
  targets = [{
    __address__ = coalesce(env("CLOUDFLARED_URL"), ""),
  }]
  forward_to      = [prometheus.relabel.cloudflared_filter.receiver]
  job_name        = "cloudflared"
  scrape_interval = "30s"
  metrics_path    = "/metrics"
}

// Filter Cloudflared metrics - keep only metrics used by dashboard
// Reduces cardinality from ~400 metrics to ~20 used metrics
prometheus.relabel "cloudflared_filter" {
  forward_to = [prometheus.relabel.add_labels.receiver]

  // Keep only metrics used by the Cloudflare Tunnel dashboard:
  // - cloudflared_tunnel_ha_connections (HA connections count)
  // - cloudflared_orchestration_config_version (config version)
  // - cloudflared_tunnel_server_locations (edge locations)
  // - cloudflared_tunnel_concurrent_requests_per_tunnel (concurrent requests)
  // - cloudflared_tunnel_total_requests (total requests counter)
  // - cloudflared_tunnel_response_by_code (response status codes)
  // - cloudflared_tunnel_request_errors (request errors)
  // - cloudflared_tunnel_tunnel_register_success (registration counter)
  // - cloudflared_proxy_connect_* (proxy connection metrics)
  // - cloudflared_rpc_client_* (RPC client metrics: latency, failures, operations)
  // - cloudflared_tcp_* (TCP session metrics)
  // - cloudflared_udp_* (UDP session metrics)
  // - cloudflared_icmp_* (ICMP metrics)
  rule {
    source_labels = ["__name__"]
    regex         = "(cloudflared_(tunnel_(ha_connections|server_locations|concurrent_requests_per_tunnel|total_requests|response_by_code|request_errors|tunnel_register_success)|orchestration_config_version|proxy_connect_.*|rpc_client_.*|tcp_.*|udp_.*|icmp_.*)|up)"
    action        = "keep"
  }
}

// Scrape Vaultwarden metrics (via vwmetrics exporter)
// Set VAULTWARDEN_URL environment variable to enable
prometheus.scrape "vaultwarden" {
  targets = [{
    __address__ = coalesce(env("VAULTWARDEN_URL"), ""),
  }]
  forward_to      = [prometheus.relabel.add_labels.receiver]
  job_name        = "vaultwarden"
  scrape_interval = "60s"  // Lower frequency - metrics update every 60s
  metrics_path    = "/metrics"
}

// Scrape Immich metrics (photo management)
// Set IMMICH_URL environment variable to enable
prometheus.scrape "immich" {
  targets = [{
    __address__ = coalesce(env("IMMICH_URL"), ""),
  }]
  forward_to      = [prometheus.relabel.add_labels.receiver]
  job_name        = "immich"
  scrape_interval = "30s"
  metrics_path    = "/metrics"
}

// Scrape Gitea metrics (git hosting)
// Set GITEA_URL and GITEA_METRICS_TOKEN environment variables to enable
prometheus.scrape "gitea" {
  targets = [{
    __address__ = coalesce(env("GITEA_URL"), ""),
  }]
  forward_to      = [prometheus.relabel.add_labels.receiver]
  job_name        = "gitea"
  scrape_interval = "30s"
  metrics_path    = "/metrics"

  // Bearer token authentication for Gitea metrics
  authorization {
    type        = "Bearer"
    credentials = coalesce(env("GITEA_METRICS_TOKEN"), "")
  }
}

// Scrape Plex metrics (via plex-media-server-exporter)
// Set PLEX_URL environment variable to enable
prometheus.scrape "plex" {
  targets = [{
    __address__ = coalesce(env("PLEX_URL"), ""),
  }]
  forward_to      = [prometheus.relabel.add_labels.receiver]
  job_name        = "plex"
  scrape_interval = "30s"
  metrics_path    = "/metrics"
}

// Scrape code-server metrics (via process-exporter)
// Set CODE_SERVER_URL environment variable to enable
// Exports process-level metrics: CPU, memory, file descriptors, threads
prometheus.scrape "code_server" {
  targets = [{
    __address__ = coalesce(env("CODE_SERVER_URL"), ""),
  }]
  forward_to      = [prometheus.relabel.add_labels.receiver]
  job_name        = "code-server"
  scrape_interval = "30s"
  metrics_path    = "/metrics"
}

// ============================================================
// ALLOY SELF-MONITORING
// ============================================================

// Scrape Alloy's own metrics for self-monitoring
// Exposes internal health, scrape statistics, and resource usage
// Key metrics:
// - alloy_component_controller_running_components (component health)
// - prometheus_target_scrape_* (scrape statistics)
// - go_memstats_*, process_* (resource usage)
prometheus.scrape "alloy_internal" {
  targets = [{
    __address__ = "localhost:12345",
  }]
  forward_to      = [prometheus.relabel.alloy_internal_filter.receiver]
  job_name        = "alloy"
  scrape_interval = "15s"
  metrics_path    = "/metrics"
}

// Filter Alloy internal metrics - keep only metrics used by dashboards and alerts
// Reduces cardinality while preserving essential monitoring capabilities
prometheus.relabel "alloy_internal_filter" {
  forward_to = [prometheus.relabel.add_labels.receiver]

  // Keep metrics for:
  // - Component health: alloy_component_* (controller, evaluation)
  // - Resources: alloy_resources_* (CPU, memory - used by dashboards)
  // - Config: alloy_config_* (hash for drift detection)
  // - Clustering: cluster_node_* (if using clustering mode)
  // - Scrape statistics: prometheus_target_*, prometheus_sd_*
  // - Loki pipeline: loki_process_*, loki_write_*
  // - Remote write: prometheus_remote_write_*
  // - Resource usage: go_*, process_* (fallback for older versions)
  // - Standard: up, scrape_*
  rule {
    source_labels = ["__name__"]
    regex         = "(alloy_(component|config|resources)_.*|cluster_node_.*|prometheus_(target|sd|remote_write)_.*|loki_(process|write)_.*|go_(memstats|gc|goroutines|threads).*|process_(cpu|resident_memory|open_fds|start_time).*|up|scrape_.*)"
    action        = "keep"
  }
}

// Add cluster and host labels to all metrics
prometheus.relabel "add_labels" {
  forward_to = [prometheus.remote_write.prometheus.receiver]

  rule {
    target_label = "cluster"
    replacement  = env("CLUSTER_NAME")
  }

  rule {
    target_label = "host"
    replacement  = env("HOSTNAME")
  }
}

// Push metrics to Prometheus via remote_write
prometheus.remote_write "prometheus" {
  endpoint {
    url = env("PROMETHEUS_URL")
  }
}
