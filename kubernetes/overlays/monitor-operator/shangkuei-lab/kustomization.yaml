---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
  - ../../../base/monitor-operator
  # SOPS-encrypted secret for Loki S3 storage (SeaweedFS)
  - secret-loki-s3.enc.yaml

labels:
  - pairs:
      app.kubernetes.io/instance: shangkuei-lab
      app.kubernetes.io/environment: lab

# shangkuei-lab cluster overlay for kube-prometheus-stack
# Storage: Mayastor (3-replica NVMe-oF) for high-performance metrics storage
#
# Storage allocation:
# - Prometheus: 200Gi on Mayastor for metrics data
# - Alertmanager: 5Gi on Mayastor for silences and notifications
# - Grafana: 10Gi on Mayastor for dashboards and plugins
patches:
  # Pin to 80.12.1 to test if 80.13.0 has metric_relabel_configs regression
  - patch: |-
      - op: replace
        path: /spec/chart/spec/version
        value: "80.12.1"
    target:
      kind: HelmRelease
      name: prometheus-operator
  - patch: |-
      # Note: Prometheus and Alertmanager configurations moved to explicit CRs
      # in monitor-cluster overlay for full GitOps control

      # Prometheus Operator resources (actual usage: 2m/21Mi)
      - op: replace
        path: /spec/values/prometheusOperator/resources
        value:
          requests:
            cpu: 25m
            memory: 64Mi
          limits:
            cpu: 200m
            memory: 256Mi

      # Grafana persistence (will be disabled below, kept for reference)
      - op: replace
        path: /spec/values/grafana/persistence
        value:
          enabled: true
          storageClassName: openebs-mayastor
          accessModes:
            - ReadWriteOnce
          size: 10Gi
      # Use SOPS-encrypted secret for Grafana admin credentials
      - op: remove
        path: /spec/values/grafana/adminPassword
      - op: add
        path: /spec/values/grafana/admin
        value:
          existingSecret: grafana-admin
          userKey: admin-user
          passwordKey: admin-password
      # Add Loki datasource for log queries (multi-tenant mode)
      - op: add
        path: /spec/values/grafana/additionalDataSources
        value:
          - name: Loki
            type: loki
            url: http://loki-gateway.monitoring.svc.cluster.local
            access: proxy
            isDefault: false
            jsonData:
              maxLines: 1000
              httpHeaderName1: X-Scope-OrgID
            secureJsonData:
              httpHeaderValue1: shangkuei-lab

      # Disable Grafana (now managed by grafana-operator)
      - op: replace
        path: /spec/values/grafana/enabled
        value: false

      # Disable exporters (now managed by standalone HelmReleases)
      - op: replace
        path: /spec/values/nodeExporter/enabled
        value: false
      - op: replace
        path: /spec/values/kube-state-metrics/enabled
        value: false
      # Force kube-state-metrics replicas to 0 as subchart doesn't respect enabled: false
      - op: add
        path: /spec/values/kube-state-metrics/replicas
        value: 0
      # Disable kube-state-metrics ServiceMonitor (using standalone chart instead)
      - op: add
        path: /spec/values/kube-state-metrics/prometheus
        value:
          monitor:
            enabled: false

      # Note: ServiceMonitors and PrometheusRules are now disabled in base configuration
      # and managed explicitly as CRs in monitor-cluster overlay for full GitOps control
    target:
      kind: HelmRelease
      name: prometheus-operator

  # Loki configuration - S3 storage with SeaweedFS backend
  - patch: |-
      # Override cluster label for recording rules and dashboards
      # Without this, recording rules use "loki" as cluster label
      - op: add
        path: /spec/values/clusterLabelOverride
        value: shangkuei-lab
      - op: add
        path: /spec/values/loki
        value:
          # Schema configuration
          schemaConfig:
            configs:
              - from: "2024-01-01"
                store: tsdb
                object_store: s3
                schema: v13
                index:
                  prefix: index_
                  period: 24h
          # S3 storage configuration (SeaweedFS)
          storage:
            type: s3
            s3:
              endpoint: http://seaweedfs-filer.seaweedfs.svc.cluster.local:8333
              s3ForcePathStyle: true
              insecure: true
            bucketNames:
              chunks: loki
              ruler: loki
              admin: loki
          # Use existing secret for S3 credentials
          storage_config:
            aws:
              s3forcepathstyle: true
      # Configure read/write/backend replicas with resources based on actual usage
      - op: add
        path: /spec/values/read
        value:
          replicas: 2
          persistence:
            storageClass: openebs-mayastor
            size: 10Gi
          # Actual usage: 11-14m CPU, 209-214Mi memory
          resources:
            requests:
              cpu: 25m
              memory: 256Mi
            limits:
              cpu: 200m
              memory: 512Mi
      - op: add
        path: /spec/values/write
        value:
          replicas: 2
          persistence:
            storageClass: openebs-mayastor
            size: 10Gi
          # Actual usage: 20-22m CPU, 153-155Mi memory
          resources:
            requests:
              cpu: 50m
              memory: 256Mi
            limits:
              cpu: 200m
              memory: 512Mi
      - op: add
        path: /spec/values/backend
        value:
          replicas: 2
          persistence:
            storageClass: openebs-mayastor
            size: 10Gi
          # Actual usage: 5-6m CPU, 155-156Mi memory
          resources:
            requests:
              cpu: 25m
              memory: 256Mi
            limits:
              cpu: 200m
              memory: 512Mi
      # Gateway configuration
      - op: add
        path: /spec/values/gateway
        value:
          replicas: 1
          # Actual usage: 4m CPU, 18Mi memory
          resources:
            requests:
              cpu: 25m
              memory: 64Mi
            limits:
              cpu: 100m
              memory: 128Mi
      # Chunked encoding for S3
      - op: add
        path: /spec/values/chunksCache
        value:
          enabled: false
      # Results cache - reduced from 1024MB to 128MB (actual usage: 31Mi)
      - op: add
        path: /spec/values/resultsCache
        value:
          allocatedMemory: 128
          resources:
            requests:
              cpu: 25m
              memory: 256Mi
            limits:
              cpu: 200m
              memory: 512Mi
      # Disable minio (we use SeaweedFS)
      - op: add
        path: /spec/values/minio
        value:
          enabled: false
      # Loki Canary resources (VPA target: 15m/32Mi, actual: 1-7m/16-19Mi)
      - op: add
        path: /spec/values/lokiCanary
        value:
          resources:
            requests:
              cpu: 15m
              memory: 32Mi
            limits:
              cpu: 100m
              memory: 64Mi
      # Compactor: enable retention processing
      - op: add
        path: /spec/values/loki/compactor
        value:
          retention_enabled: true
          retention_delete_delay: 2h
          delete_request_store: s3
      # Limits: retention 31 days, deletion API, query tuning
      # Note: deletion_mode moved from compactor to limits_config in Loki 3.x
      - op: add
        path: /spec/values/loki/limits_config
        value:
          retention_period: 744h
          deletion_mode: filter-and-delete
          max_query_parallelism: 32
          max_query_series: 10000
      # Enable ServiceMonitor for Prometheus scraping (required for Loki dashboards)
      # Note: Path is monitoring.serviceMonitor, not serviceMonitor
      - op: add
        path: /spec/values/monitoring/serviceMonitor
        value:
          enabled: true
          labels:
            release: kube-prometheus-stack
            prometheus.io/scrape-by: alloy
          # Relabelings to set cluster label for dashboard compatibility
          # Note: job label is already in namespace/service format (e.g., monitoring/loki-backend)
          #       from the ServiceMonitor, no modification needed
          relabelings:
            # Set cluster label for multi-cluster support
            - targetLabel: cluster
              replacement: shangkuei-lab
          # Loki cardinality reduction (~9.9K â†’ ~2K series)
          # Preserve only metrics used by Loki dashboards (simple-scalable mode)
          metricRelabelings:
            # Override cluster label from "loki" to "shangkuei-lab" for dashboard compatibility
            - sourceLabels: [cluster]
              targetLabel: cluster
              replacement: shangkuei-lab
            # Drop internal histogram buckets not used in dashboards (~5K series)
            # Keep: loki_request_duration_seconds (recording rules), loki_s3_request_duration_seconds (S3 latency),
            #       loki_ingester_chunk_*, loki_boltdb_shipper_*
            - sourceLabels: [__name__]
              action: drop
              regex: loki_(response_message_bytes|request_message_bytes|logql_querystats_.*|index_request_duration_seconds|kv_request_duration_seconds|cache_value_size_bytes|frontend_query_range_duration_seconds|ingester_client_request_duration_seconds|index_gateway_request_duration_seconds|cache_request_duration_seconds|tsdb_shipper_.*|log_flushes|bytes_per_line)_bucket
            # Drop internal metrics not used in dashboards
            - sourceLabels: [__name__]
              action: drop
              regex: loki_(ring_.*|inflight_requests|bloom_.*)
            # Drop canary metrics (testing only)
            - sourceLabels: [__name__]
              action: drop
              regex: loki_canary_.*
            # Drop memberlist/gossip metrics (internal)
            - sourceLabels: [__name__]
              action: drop
              regex: loki_memberlist_.*
            # Drop query frontend/scheduler internals (not in simple-scalable dashboards)
            - sourceLabels: [__name__]
              action: drop
              regex: loki_(query_frontend_.*|query_scheduler_.*|querier_.*)
            # Drop rate store metrics (rate limiting internals)
            - sourceLabels: [__name__]
              action: drop
              regex: loki_rate_store_.*
            # Drop index gateway metrics (internal)
            - sourceLabels: [__name__]
              action: drop
              regex: loki_index_gateway_.*
            # Drop WAL and checkpoint metrics (not used)
            - sourceLabels: [__name__]
              action: drop
              regex: loki_ingester_(wal_.*|checkpoint_.*)
            # Drop Kafka distributor metrics (not using Kafka)
            - sourceLabels: [__name__]
              action: drop
              regex: loki_distributor_kafka_.*
            # Drop store and cache metrics (internal)
            - sourceLabels: [__name__]
              action: drop
              regex: loki_(store_.*|cache_.*|memcache_.*)
            # Drop Azure blob metrics (not using Azure)
            - sourceLabels: [__name__]
              action: drop
              regex: loki_azure_.*
      # Disable Loki helm chart recording rules and alerts
      # We deploy loki-mixin rules instead (monitor-cluster/loki-mixin-recording-rules.yaml)
      # which include the cluster dimension required by loki-mixin dashboards
      - op: add
        path: /spec/values/monitoring/rules
        value:
          enabled: false
          alerting: false
      # Configure S3 credentials from secret
      - op: add
        path: /spec/valuesFrom
        value:
          - kind: Secret
            name: loki-s3-credentials
            valuesKey: access_key_id
            targetPath: loki.storage.s3.accessKeyId
          - kind: Secret
            name: loki-s3-credentials
            valuesKey: access_key_secret
            targetPath: loki.storage.s3.secretAccessKey
    target:
      kind: HelmRelease
      name: loki

  # Standalone prometheus-node-exporter configuration
  - patch: |-
      # Resources based on actual usage (1-13m/9-26Mi)
      - op: add
        path: /spec/values/resources
        value:
          requests:
            cpu: 25m
            memory: 32Mi
          limits:
            cpu: 100m
            memory: 64Mi
      - op: add
        path: /spec/values/prometheus/monitor/additionalLabels
        value:
          prometheus.io/scrape-by: alloy
      - op: add
        path: /spec/values/prometheus/monitor/relabelings
        value:
          - targetLabel: cluster
            replacement: shangkuei-lab
          # Rename job to match kube-prometheus dashboards expectations
          - targetLabel: job
            replacement: node-exporter
          # Add node label for joining with kubelet/cadvisor metrics
          - sourceLabels: [__meta_kubernetes_pod_node_name]
            targetLabel: node
      - op: add
        path: /spec/values/prometheus/monitor/metricRelabelings
        value:
          # Drop node scrape collector metrics (~290 series) - internal
          - sourceLabels: [__name__]
            action: drop
            regex: node_scrape_.*
    target:
      kind: HelmRelease
      name: prometheus-node-exporter

  # Standalone kube-state-metrics configuration
  - patch: |-
      # Resources based on actual usage (2m/46Mi)
      - op: add
        path: /spec/values/resources
        value:
          requests:
            cpu: 25m
            memory: 64Mi
          limits:
            cpu: 100m
            memory: 128Mi
      - op: add
        path: /spec/values/prometheus/monitor/additionalLabels
        value:
          prometheus.io/scrape-by: alloy
      # Honor labels from kube-state-metrics to avoid exported_* prefix
      # This makes namespace/pod/container labels work correctly in dashboards
      - op: add
        path: /spec/values/prometheus/monitor/honorLabels
        value: true
      - op: add
        path: /spec/values/prometheus/monitor/relabelings
        value:
          - targetLabel: cluster
            replacement: shangkuei-lab
      - op: add
        path: /spec/values/prometheus/monitor/metricRelabelings
        value:
          # Drop kube_pod_labels and kube_pod_annotations (very high cardinality)
          - sourceLabels: [__name__]
            regex: kube_(pod_annotations|pod_labels|configmap_annotations|secret_annotations|service_annotations|node_labels)
            action: drop
          # Drop high-cardinality pod metrics not used in alerts/dashboards (~1K series)
          - sourceLabels: [__name__]
            regex: kube_pod_(tolerations|status_reason|status_qos_class)
            action: drop
          # Drop 'uid' label (62 unique values) - rarely queried directly
          - regex: ^uid$
            action: labeldrop
          # Drop 'created_by_name' label (35 unique values) - rarely queried
          - regex: ^created_by_name$
            action: labeldrop
          # Drop 'container_id' label (187 unique values) - cgroup ID, not queried
          - regex: ^container_id$
            action: labeldrop
          # Drop 'image_id' label - SHA256 digest, never queried directly
          - regex: ^image_id$
            action: labeldrop
          # Drop 'image_spec' label - redundant with 'image' label
          - regex: ^image_spec$
            action: labeldrop
    target:
      kind: HelmRelease
      name: kube-state-metrics

  # Grafana Operator configuration
  # Note: grafana-operator Helm chart uses 'additionalLabels' not 'labels'
  - patch: |-
      # Resources (actual usage: 2m/25Mi)
      - op: replace
        path: /spec/values/resources
        value:
          requests:
            cpu: 25m
            memory: 64Mi
          limits:
            cpu: 200m
            memory: 256Mi
      - op: add
        path: /spec/values/serviceMonitor/additionalLabels
        value:
          prometheus.io/scrape-by: alloy
      - op: add
        path: /spec/values/serviceMonitor/relabelings
        value:
          - targetLabel: cluster
            replacement: shangkuei-lab
    target:
      kind: HelmRelease
      name: grafana-operator
